{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic\n",
    "\n",
    "## Can we leverage deep learning on irregular domains to save lifes?\n",
    "\n",
    "---\n",
    "\n",
    "*Teo Stocco, Pierre-Alexandre Lee, Yves Lamonato, Charles Thiebaut*, [EPFL](https://epfl.ch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Network Tour of Data Science](https://github.com/mdeff/ntds_2017) final project:<br>\n",
    "This notebook contains a detailed overview through the whole project with all essential parts. As this work required several attempts and exploration, only relevant parts are kept here. You can however access their individual and unguided research notebooks in the `lab` folder.<br>\n",
    "This project was **not shared** with any other class.\n",
    "\n",
    "[Binder access](https://mybinder.org/v2/gh/zifeo/Titanic/) | [nbviewer access](https://nbviewer.jupyter.org/github/zifeo/Titanic/blob/master/project.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "@teo\n",
    "- spell check plugin\n",
    "- isolate code blocks in functions in separate Python module\n",
    "- notebook toc plugin\n",
    "- nicer README \n",
    "- bundle tous les imports en haut\n",
    "- verify gcnn comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "Icebergs and ships do not get well along each other. To avoid dramatic events such as the one that happened a century ago, we aim at helping a noble quest: differentiating icebergs and ships based on radar data to see whether any\n",
    "iceberg is drifting away and might cross the road of a ship.\n",
    "\n",
    "<br>\n",
    "\n",
    "|© Statoil/C-CORE - Icebergs and ships examples|\n",
    "|-|\n",
    "|![](./img/statoil-ccore.png)|\n",
    "\n",
    "<br>\n",
    "\n",
    "This remote sensing measurements can be performed either by planes or by satellites. The second can provide radar information up to 14 time a day as in the case of [Sentinel-1](https://fr.wikipedia.org/wiki/Sentinel-1). The C-Band radar manages to capture data in numerous conditions (e.g. darkness, rain, cloud, fog, etc.) and measures the energy reflected back called backscatter (Torres et al, 2012). Those data can later be analyzed and used to clear out potential collision between icebergs and ships. \n",
    "\n",
    "Building on the top of recent advances in the field of signal processing on graphs (Schuman et al., 2013) and deep learning on irregular domains (Bronstein et al., 2017), we investigate the performance of standard machine learning methods and the relevance of graph based convolutional neural networks to perform binary classification in this specific case (layered data). The new method provide a convenient way of getting rotational invariance over the data (Defferrard et al., 2017) and set up a flexible framework for structured pooling. \n",
    "\n",
    "As the pooling operations require adequate aggregation by coarsening the graph between layers, we experiment how this framework can be exploited through various processes: Graclus multilevel algorithm and algebraic multigrid techniques. We further extend by comparing on different cases: grid graph, knn graph and wrapped-knn graph. Finally, we show that one can take advantage of graphs to defined structured pooling.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add gcnn module\n",
    "sys.path.append('..')\n",
    "\n",
    "# bigger figure\n",
    "plt.rcParams['figure.figsize'] = 18, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed for reproducability\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope to specfic gpu\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data source\n",
    "\n",
    "The dataset is provided by Statoil, an oil and gas compagny, and C-CORE, a monitoring company using computer vision to keep naval operations safe and efficient. It was released on Kaggle for prediction competition in late 2017. The full dataset contains `10'028` iceberg or ship cases with only `1'604` labelled. Some of the test images were computer generated to avoid hand labelling in the competition. As we will only focus on labelled one, this should not matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "For each case, the following covariates are provided. Two radar bands of a resolution `75x75` corresponding to  \"transmitted and received horizontally\" (HH) and \"transmitted horizontally and received vertically\" (HV) data. One additional feature (angle) and the label.\n",
    "\n",
    "<br>\n",
    "\n",
    "| Feature | Description | Type | Has N/A | Comment |\n",
    "| - | - | - | - | - |\n",
    "| id | image identifier| String | No | |\n",
    "| band_1 | horizontal plane | Float array | No| HH |\n",
    "| band_2 | vertical plane | Float array | No| HV |\n",
    "| inc_angle | measurement angle | Float| Yes(~10%) | Unit in degrees |\n",
    "| is_iceberg | label (iceberg or not)| Boolean (0/1)| No| |\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.datasets import load_icebergs\n",
    "\n",
    "# train = all labelled cases from Kaggle\n",
    "measures = load_icebergs('train')\n",
    "measures.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "To get a first qualitative insight, let's have a look at an example of an iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_iceberg = measures.iloc[5]\n",
    "example_iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn import viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bands(example_iceberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bands_3d(example_iceberg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some remarks:\n",
    "- data seems to be centered\n",
    "- this example is round, however checking further cases reveal that they vary a lot in size and shape\n",
    "- the two bands can have a noticeable difference in intensity\n",
    "- noise presence\n",
    "\n",
    "---\n",
    "\n",
    "Let's now look at an example of a ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ship = measures.iloc[0]\n",
    "example_ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bands(example_ship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bands_3d(example_ship, angle=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some remarks:\n",
    "\n",
    "- data also seems to be centered\n",
    "- this example has a ship-like shape, however checking further cases reveal that they also vary a lot\n",
    "- noise does not seem to be different\n",
    "\n",
    "---\n",
    "\n",
    "What about the label distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('iceberg distribution')\n",
    "measures.groupby(measures.is_iceberg).is_iceberg.count().plot.barh();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the two classes across the data is quite even (~53% of ships, ~47% of icebergs).\n",
    "\n",
    "---\n",
    "\n",
    "And what about distribution of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(measures.inc_angle.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Distribution of angles from the bands\")\n",
    "sns.distplot(measures.inc_angle.dropna());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "133 of the measures have an absent angle, so we just ignore them for the correlation coefficient and they will be replaced later during the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Iceberg bands distribution')\n",
    "sns.distplot(example_iceberg.band_1, label='band 1')\n",
    "sns.distplot(example_iceberg.band_2, label='band 2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Ship bands distribution')\n",
    "sns.distplot(example_ship.band_1, label='band 1')\n",
    "sns.distplot(example_ship.band_2, label='band 2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bands appear as bell-shapes and their difference might contain valuable information. This is why the same scaler will be used later to perserve their eventual gaps.\n",
    "\n",
    "---\n",
    "\n",
    "T-SNE allows to represent non-linear high dimensionality data on only two dimensions which can be easily visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca can speed up T-SNE and suppress some residual noise\n",
    "pca50 = PCA(n_components=50).fit_transform(np.c_[np.stack(measures.band_1), np.stack(measures.band_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented as slow tsne = TSNE(n_components=2, perplexity=60).fit_transform(pca50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented as slow plt.title('T-NSE icebergs vs ships')\n",
    "# commented as slow plt.scatter(tsne[measures.is_iceberg == 1, 0], tsne[measures.is_iceberg == 1, 1], label='icebergs')\n",
    "# commented as slow plt.scatter(tsne[measures.is_iceberg == 0, 0], tsne[measures.is_iceberg == 0, 1], label='ships')\n",
    "# commented as slow plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The T-SNE does not show any obvious underlying clue (e.g. clusters) after some parameters tweaking. This problem is thus unlikely to be resolved by simple classifiers as such as k-nearest neighbors. It can be noticed that some regions are \"closely\" shared among icebergs and ships whereas other are more distinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypes\n",
    "\n",
    "One interesting first point is to look whether there are some prototypes (distinct primitive shapes). This allows to gather insights that will be later useful if there is any imbalance between icebergs and ships. For example, one could try to cluster similar band 1 together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_proto = 9 #initial 4\n",
    "kmeans = KMeans(n_clusters=n_proto).fit((np.stack(measures.band_1) + np.stack(measures.band_2)) / 2)\n",
    "kmeans_centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, center in enumerate(kmeans_centers):\n",
    "    plt.subplot(1, 4, i % 4 + 1)\n",
    "    plt.imshow(center.reshape(75, 75))\n",
    "    if i % 4 == 3:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# per cluster, per label, counts\n",
    "measures.is_iceberg.groupby(kmeans.labels_).apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the first cluster seems to have a majority of ships, prototypes are unlikely to be specific to a given label in such high dimensions (partly because of curse of dimensionality). However this confirms the hypothesis on centered data and shows the prescence of some scatter reflections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of clusters looks to improves the classification between the two. This is to be expected and different not that noticeable, we should cosider other methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Preprocessing\n",
    "\n",
    "There is no particular pre-processing to do (except from scaling and managing the N/A), since the data is already nicely formatted. Also since the image are already quite small (75x75), in particular the central zone of interest, we choose not to smooth it (with a gaussian filter by example) in order not to loose important details. Noise might thus have a strong impact but this will also allow to see how graph based learning manage that issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test splits\n",
    "\n",
    "Using the same `random_state`, the data is split the same way here as in the others notebooks where we tuned the models using cross-validation on the following train split. The test split is only used to compare final models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random state is very important as they same split can be used in other notebooks\n",
    "train, test = train_test_split(range(len(measures)), test_size=0.15, stratify=measures.is_iceberg, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we did a fair (stratified) split for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.iloc[train].is_iceberg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.iloc[test].is_iceberg.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and scaling\n",
    "\n",
    "The angle is likely to play an important role and will be replaced by `0` to stay distinct from known values. The bands will be scaled between 0 and 1 so that they preserve their respective mean (location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_scaler = MinMaxScaler()\n",
    "angle_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, e in measures.iloc[train].iterrows():\n",
    "    band_scaler.partial_fit(e.band_1.reshape(1, -1))\n",
    "    band_scaler.partial_fit(e.band_2.reshape(1, -1))\n",
    "    \n",
    "angle_scaler.fit(measures.iloc[train].inc_angle.dropna().values.reshape(-1, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usable data contains the two bands, the angle and the target (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = np.stack([\n",
    "    band_scaler.transform(np.stack(measures.band_1)),\n",
    "    band_scaler.transform(np.stack(measures.band_2)),\n",
    "], axis=1).reshape(-1, 2, 75, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = angle_scaler.transform(measures.inc_angle.fillna(0).values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = measures.is_iceberg.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Graphs\n",
    "\n",
    "To prepare later graph-based learning, various grids and coarsening methods are showcased on small graphs. The motivation for using this dataset is to see how the two bands can be modelled as irregular domains versus classical convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn import graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical 2D grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grid = nx.grid_graph([5, 5])\n",
    "nx.draw(small_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Knn 2D grid\n",
    "\n",
    "Each node is connected to its k-nearest neighbors (or more in case of equality, this is due to the interpolation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_knn = graph.knn(graph.grid_coordinates(5), k=8, metric='cityblock')\n",
    "nx.draw(small_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap-around Knn 2D grid\n",
    "\n",
    "Same as above, with wrap-around borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_wraps = graph.kwraps(5, kd=1)\n",
    "nx.draw(small_wraps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical 3D grid\n",
    "\n",
    "With two levels of depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grid3d = nx.grid_graph([5, 5, 2])\n",
    "nx.draw(small_grid3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knn 3D grid\n",
    "\n",
    "With two levels of depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_knn3d = graph.knn3d(graph.grid_coordinates(5), k=8, metric='cityblock', d=2)\n",
    "nx.draw(small_knn3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D wrap-around grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_wraps3d = graph.kwraps3d(5, kd=1, d=2)\n",
    "nx.draw(small_wraps3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze how those graph compare in quantitative terms (e.g. connectivity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_graphs = [small_grid, small_knn, small_wraps, small_grid3d, small_knn3d, small_wraps3d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.mean(list(dict(g.degree()).values())) for g in small_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented as slow [nx.average_node_connectivity(g) for g in small_graphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Giant Compoment Analysis : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of nodes of the giant component of each graph above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant_small_grid = max(nx.connected_component_subgraphs(small_grid), key=len)\n",
    "giant_small_knn = max(nx.connected_component_subgraphs(small_knn), key=len)\n",
    "giant_small_wraps = max(nx.connected_component_subgraphs(small_wraps), key=len)\n",
    "giant_small_grid3d = max(nx.connected_component_subgraphs(small_grid3d), key=len)\n",
    "giant_small_knn3d = max(nx.connected_component_subgraphs(small_wraps3d), key=len)\n",
    "giant_small_wraps3d = max(nx.connected_component_subgraphs(small_knn3d), key=len)\n",
    "print(\"Small grid : \",len(giant_small_grid.nodes()))\n",
    "print(\"Small KNN : \",len(giant_small_knn.nodes()))\n",
    "print(\"Small wraps : \",len(giant_small_wraps.nodes()))\n",
    "print(\"Small grid 3D : \",len(giant_small_grid3d.nodes()))\n",
    "print(\"Small KNN 3D : \",len(giant_small_knn3d.nodes()))\n",
    "print(\"Small wraps 3D : \",len(giant_small_wraps3d.nodes()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments : as the two groups of graphs are created with the same parameters, the above results are to be expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Clustering : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Small grid : \",nx.average_clustering(small_grid))\n",
    "print(\"Small KNN : \",nx.average_clustering(small_knn))\n",
    "print(\"Small wraps : \",nx.average_clustering(small_wraps))\n",
    "print(\"Small grid 3D : \",nx.average_clustering(small_grid3d))\n",
    "print(\"Small KNN 3D : \",nx.average_clustering(small_knn3d))\n",
    "print(\"Small wraps 3D : \",nx.average_clustering(small_wraps3d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments : The symetric of the grid meaning that the average clustering is 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarsening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduced graphs can be visualized coarsening step by coarsening step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graclus\n",
    "\n",
    "[Graclus](http://www.cs.utexas.edu/users/dml/Software/graclus.html) is a fast graph clustering software that computes normalized or ratio cut. It might add some nodes to match a reduction by 2. Implementation is courtesy of Michael Defferrard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import graclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graclus_levels, _ = graclus.coarsen(nx.adjacency_matrix(small_grid), levels=2, self_connections=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_graph_steps(graclus_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algebraic multigrid\n",
    "\n",
    "@PA: Small description/principe + lien externe vers références"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import amg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grid = nx.grid_graph([4, 4, 1])\n",
    "small_dist = nx.adjacency_matrix(small_grid)\n",
    "graphs, perm = amg.coarsen_amg(small_dist, levels=3, self_connections=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs:\n",
    "    plt.subplot(121)\n",
    "    plt.spy(g.todense())\n",
    "    plt.subplot(122)\n",
    "    nx.draw(nx.from_numpy_array(g.todense()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kron reduction\n",
    "\n",
    "@PA: Small description/principe + lien externe vers références\n",
    "+ expliquer pourquoi ça marche ou remove? on peut aussi le mentionner dans la présentation uniquement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import kron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kron_levels = kron.graph_multiresolution(sp.sparse.csr_matrix(nx.adjacency_matrix(small_grid)), levels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in kron_levels:\n",
    "    g.set_coordinates()\n",
    "    g.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum spanning tree\n",
    "\n",
    "@Yves: Small description/principe + lien externe vers références\n",
    "+ expliquer pourquoi ça marche ou remove? on peut aussi le mentionner dans la présentation uniquement ?\n",
    "\n",
    "We tried to use a MST (Maximum Spaning Tree) based downsampling. The idea is to find the max-cut to separate our nodes in a bipartite graph, but this problem is in general NP-hard.  \n",
    "\n",
    "So to find a high-cut in a fast way, we took some inspiration of this paper (Nguyen, \"Downsampling of Signals on Graphs\n",
    "Via Maximum Spanning Trees\") and implemented our solution. The algorithm goes this way : find the MST of our connected (by construction) graph, using Prim's algorithm for example, then randomly choose one node to be the root node, and from there compute the distance of each node to the root node in the MST. \n",
    "\n",
    "From there, only keep the nodes which distance to the root is even. In the resulting graph, the weight between the nodes we kept are computed from the two edges that originally connected them in the MST.  \n",
    "\n",
    "However, we didn't use this method since after the splitting we couldn't find exactly which nodes to cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import mst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mst_levels = mst.mst(nx.adjacency_matrix(small_grid).todense(), levels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_graph_steps(mst_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the minimum spanning tree and kron based methods, we haven't found an efficient way to know which nodes are being clustered together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Models\n",
    "\n",
    "Starting from standards models, classical convolution will be tuned and serve as a reference against graph convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame([], columns=['name', 'accuracy', 'precision', 'recall', 'f1']).set_index('name')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flat_features = np.c_[bands.reshape(-1, 2 * 75 * 75), angles.reshape(-1, 1)]\n",
    "flat_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard methods\n",
    "\n",
    "Naive, Knn and logistic regression. Parameters were tuned in other notebooks on same training set (cross-validated) and reported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier().fit(flat_features[train], targets[train].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.utils import score_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['baseline'] = score_classification(targets[test], dummy.predict(flat_features[test]))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=6).fit(flat_features[train], targets[train].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['knn'] = score_classification(targets[test], knn.predict(flat_features[test]))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(C=0.1).fit(flat_features[train], targets[train].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['logistic'] = score_classification(targets[test], logistic.predict(flat_features[test]))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution baseline\n",
    "\n",
    "Inspired from LeNet-5, the following architecture is used as a baseline for convolution.\n",
    "\n",
    "@qqun draw.io graph pour expliquer ? ou https://github.com/pytorch/pytorch/issues/2001\n",
    "+ expliquer pourquoi pas de dropout etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_t = torch.from_numpy(bands).float()\n",
    "angles_t = torch.from_numpy(angles).float()\n",
    "targets_t = torch.from_numpy(targets).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.nets import BaselineCNN\n",
    "from gcnn.utils import NNplusplus\n",
    "from skorch import NeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skorch provides a sklearn interface over pytorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = NNplusplus(\n",
    "    BaselineCNN,\n",
    "    use_cuda=cuda,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=nn.BCEWithLogitsLoss, \n",
    "    batch_size=50,\n",
    "    max_epochs=15,\n",
    "    lr=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.utils import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_fit_predict(name, model, bands, angles, targets):\n",
    "    \n",
    "    # fit model with bands and angles\n",
    "    train_features = dict(x=bands[train], x2=angles[train])\n",
    "    model.fit(train_features, targets[train])\n",
    "    print()\n",
    "    \n",
    "    # need logit for prediction as it is included within optimizer\n",
    "    test_features = dict(x=bands[test], x2=angles[test])\n",
    "    preds = model.predict_proba(test_features)\n",
    "    preds = sigmoid(preds).round()\n",
    "    \n",
    "    # save score\n",
    "    scores.loc[name] = score_classification(targets[test], preds)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fit_predict('conv', cnn, bands_t, angles_t, targets_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch as th\n",
    "from collections import OrderedDict\n",
    "def summary(input_size, model):\n",
    "        def register_hook(module):\n",
    "            def hook(module, input, output):\n",
    "                class_name = str(module.__class__).split('.')[-1].split(\"'\")[0]\n",
    "                module_idx = len(summary)\n",
    "\n",
    "                m_key = '%s-%i' % (class_name, module_idx+1)\n",
    "                summary[m_key] = OrderedDict()\n",
    "                summary[m_key]['input_shape'] = list(input[0].size())\n",
    "                summary[m_key]['input_shape'][0] = -1\n",
    "                summary[m_key]['output_shape'] = list(output.size())\n",
    "                summary[m_key]['output_shape'][0] = -1\n",
    "\n",
    "                params = 0\n",
    "                if hasattr(module, 'weight'):\n",
    "                    params += th.prod(th.LongTensor(list(module.weight.size())))\n",
    "                    if module.weight.requires_grad:\n",
    "                        summary[m_key]['trainable'] = True\n",
    "                    else:\n",
    "                        summary[m_key]['trainable'] = False\n",
    "                if hasattr(module, 'bias'):\n",
    "                    params +=  th.prod(th.LongTensor(list(module.bias.size())))\n",
    "                summary[m_key]['nb_params'] = params\n",
    "                \n",
    "            if(not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList) and not (module == model)):\n",
    "                hooks.append(module.register_forward_hook(hook))\n",
    "        \n",
    "        # check if there are multiple inputs to the network\n",
    "        if isinstance(input_size[0], (list, tuple)):\n",
    "            x = [Variable(th.rand(1,*in_size)) for in_size in input_size]\n",
    "        else:\n",
    "            x = Variable(th.rand(1,*input_size))\n",
    "\n",
    "        # create properties\n",
    "        summary = OrderedDict()\n",
    "        hooks = []\n",
    "        # register hook\n",
    "        #model.apply(register_hook)\n",
    "        # make a forward pass\n",
    "        model(x)\n",
    "        # remove these hooks\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(bands_t.shape,cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@teo share same parameter, cheby vs fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "from scipy.sparse import csgraph\n",
    "from gcnn.nets import GraphCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarsen_permute(graph, bands, algo=graclus):\n",
    "    \n",
    "    # node should be ordered in a way corresponding to bands data order\n",
    "    if type(graph) != np.ndarray:\n",
    "        grid = nx.adjacency_matrix(graph, sorted(graph.nodes))\n",
    "    else:\n",
    "        grid = graph\n",
    "        \n",
    "    # coarsen grid\n",
    "    grid = sp.sparse.csr.csr_matrix(grid)\n",
    "    laps, perms = algo.coarsen(grid, levels=3, self_connections=False)\n",
    "    print()\n",
    "    \n",
    "    # compute normed laplacian\n",
    "    laps = [csgraph.laplacian(g, normed=True) for g in laps[:-1]]\n",
    "    \n",
    "    # update data accordingly\n",
    "    pbands = graclus.perm_data(bands, perms)\n",
    "    pbands = torch.from_numpy(pbands).float()\n",
    "    \n",
    "    return laps, pbands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_model(laps):\n",
    "    l0 = torch.from_numpy(laps[0].todense()).float()\n",
    "    l2 = torch.from_numpy(laps[2].todense()).float()\n",
    "    \n",
    "    if cuda:\n",
    "        l0 = l0.cuda()\n",
    "        l2 = l2.cuda()\n",
    "    \n",
    "    return NeuralNet(\n",
    "        GraphCNN,\n",
    "        module__k=25, # if k None then fourier mode and should send fourier basis instead of laplacian\n",
    "        module__lf0=l0,\n",
    "        module__lf2=l2,\n",
    "        use_cuda=cuda,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        criterion=nn.BCEWithLogitsLoss, \n",
    "        batch_size=50,\n",
    "        max_epochs=15,\n",
    "        lr=0.001,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_single_band(name, grid, size=75, algo=graclus):\n",
    "    laps, pbands = coarsen_permute(grid, bands[:, 0, :, :].reshape(-1, size * size), algo)\n",
    "    return score_fit_predict(name, gc_model(laps), pbands, angles_t, targets_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_both_bands(name, grid, size=75, algo=graclus):\n",
    "    laps, pbands = coarsen_permute(grid, bands.reshape(-1, 2 * size * size), algo)\n",
    "    return score_fit_predict(name, gc_model(laps), pbands, angles_t, targets_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each graph has a particular nodes ordering, precaution need to be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_single_band('gcnn_grid', nx.grid_graph([75, 75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_single_band('gcnn_knn', graph.knn(graph.grid_coordinates(75), k=8, metric='cityblock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_single_band('gcnn_kwraps', graph.kwraps(75, kd=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_both_bands('gcnn_grid_2', nx.grid_graph([75, 75, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of memory error\n",
    "# gc_both_bands('gcnn_knn_2', graph.knn3d(graph.grid_coordinates(75), k=4, metric='cityblock', d=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of memory error\n",
    "# gc_both_bands('gcnn_kwraps_2', graph.kwraps3d(75, kd=1, d=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph convolution with amg\n",
    "\n",
    "@PA remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import amg\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csgraph\n",
    "import skimage.transform as transform\n",
    "from scipy import spatial\n",
    "from pygsp import graphs as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_backup = measures.copy()\n",
    "\n",
    "def rescale(x, p=0.5):\n",
    "    return transform.rescale(x.reshape(75, 75), p, mode='constant').reshape(-1, 1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.band_1 = measures_backup.band_1.apply(rescale, 2/3)\n",
    "measures.band_2 = measures_backup.band_2.apply(rescale, 2/3)\n",
    "\n",
    "gc_single_band('gcnn_amg', nx.grid_graph([50, 50, 1]), size=50, algo=amg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_single_band('gcnn_amg_wrap', graph.kwraps(50, kd=1), size=50, algo=amg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.band_1 = measures_backup.band_1.apply(rescale, 0.5)\n",
    "measures.band_2 = measures_backup.band_2.apply(rescale, 0.5)\n",
    "gc_both_bands('gcnn_amg2', nx.grid_graph([38, 38, 2]), size=38, algo=amg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking into account the spectre of the images\n",
    "One interesting thing to do is to take into account the spectre of the images when coarsening the graph. Taking of of the prototypes computed above and AMG, the idea is to coarsen the graph such that we keep most of the nodes at the center and cluster the nodes near the borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = graph.knn(graph.grid_coordinates(50), k=2, metric='cityblock')\n",
    "image = transform.rescale(kmeans_centers[0].reshape(75, 75), 2/3, mode='constant')\n",
    "#e = measures.iloc[5]\n",
    "#image = transform.rescale(np.c_[e.band_1.reshape(75, 75), e.band_2.reshape(75, 75)], 0.5, mode='constant')\n",
    "image_grid = spatial.distance.squareform(spatial.distance.pdist(image.reshape(-1, 1), metric='euclidean'))\n",
    "gc_knn_laps = nx.adjacency_matrix(g, sorted(g.nodes)) > 0\n",
    "mask = gc_knn_laps.todense() == 0\n",
    "image_grid[np.where(mask)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.band_1 = measures_backup.band_1.apply(rescale, 2/3)\n",
    "measures.band_2 = measures_backup.band_2.apply(rescale, 2/3)\n",
    "gc_single_band('gcnn_amg_image', image_grid, size=50, algo=amg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Evaluation\n",
    "\n",
    "@qqun graph des résultats\n",
    "\n",
    "et interprète les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Conclusion\n",
    "\n",
    "\n",
    "Even though can be further trained, we fixed to better compare and analyze\n",
    "\n",
    "Low data, what if more data ?\n",
    "hard to converge\n",
    "need new abstraction or library as operation can quickly become very complex (transforming data into grid on correct points)\n",
    "\n",
    "\n",
    "### Improvements\n",
    "\n",
    "- sparse operations\n",
    "- more parameters setting with std in scores reported\n",
    "- speedup and complexity analysis\n",
    "- graph deconv (view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 References\n",
    "\n",
    "- TORRES, Ramon, SNOEIJ, Paul, GEUDTNER, Dirk, et al. GMES Sentinel-1 mission. Remote Sensing of Environment, 2012, vol. 120, p. 9-24.\n",
    "- SHUMAN, David I., NARANG, Sunil K., FROSSARD, Pascal, et al. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine, 2013, vol. 30, no 3, p. 83-98.\n",
    "- BRONSTEIN, Michael M., BRUNA, Joan, LECUN, Yann, et al. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 2017, vol. 34, no 4, p. 18-42.\n",
    "- DEFFERRARD, Michaël, BRESSON, Xavier, et VANDERGHEYNST, Pierre. Convolutional neural networks on graphs with fast localized spectral filtering. In : Advances in Neural Information Processing Systems. 2016. p. 3844-3852.\n",
    "- NGUYEN Ha Q., DO Minh N, et al. Downsampling of Signal on Graphs Via Maximum Spanning Trees. IEEE Transactions on Signal Processing, 2015, vol. 63, no 1.\n",
    "- DORFLER Florain, BULLO Francesco. Kron reduction of graphs with applications to electrical networks. 2011."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
